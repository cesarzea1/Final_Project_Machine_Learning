{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eb135b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Read the file for the model\n",
    "model_df = pd.read_csv(r'C:\\Users\\pola_\\OneDrive\\Documents\\Cesar\\Data-bootcamp-UoT\\Lessons\\week 23\\final project\\Final_Project_Machine_Learning\\Resources\\clean_sampled_US_Accidents_for_model.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b44f61c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a random 10% of the data\n",
    "model_df = model_df.sample(frac=0.1, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcec87c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'ID' and 'County' columns\n",
    "model_df = model_df.drop(['ID', 'County'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adbe36c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encode categorical variables\n",
    "categorical_columns = ['State', 'Weather_Condition', 'Astronomical_Twilight']\n",
    "model_df = pd.get_dummies(model_df, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee36103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant variable based on correlations analysis\n",
    "model_df = model_df.drop('Astronomical_Twilight_Day', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6a82e3",
   "metadata": {},
   "source": [
    "###  I tried eliminating the states to see if the models fit better, but that was not the case.  Leaving the code for reference. \n",
    "\n",
    "#### List all columns that start with 'State_'\n",
    "#### state_columns = [col for col in model_df.columns if col.startswith('State_')]\n",
    "#### model_df = model_df.drop(state_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b314ed75",
   "metadata": {},
   "source": [
    "###  I tried eliminating columns that might not have impact on  accidents.  Leaving the code for reference.\n",
    "\n",
    "#### model_df = model_df.drop(['Amenity', 'No_Exit', 'Railway', 'Station'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3831f1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Start_Time' to datetime and extract 'Hour'\n",
    "model_df['Start_Time'] = pd.to_datetime(model_df['Start_Time'])\n",
    "model_df['Hour'] = model_df['Start_Time'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c19a942e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 353510 entries, 3030768 to 30760\n",
      "Data columns (total 90 columns):\n",
      " #   Column                             Non-Null Count   Dtype  \n",
      "---  ------                             --------------   -----  \n",
      " 0   Severity                           353510 non-null  int64  \n",
      " 1   Start_Lat                          353510 non-null  float64\n",
      " 2   Start_Lng                          353510 non-null  float64\n",
      " 3   Distance(mi)                       353510 non-null  float64\n",
      " 4   Temperature(F)                     353510 non-null  float64\n",
      " 5   Humidity(%)                        353510 non-null  float64\n",
      " 6   Pressure(in)                       353510 non-null  float64\n",
      " 7   Visibility(mi)                     353510 non-null  float64\n",
      " 8   Wind_Speed(mph)                    353510 non-null  float64\n",
      " 9   Precipitation(in)                  353510 non-null  float64\n",
      " 10  Amenity                            353510 non-null  bool   \n",
      " 11  Bump                               353510 non-null  bool   \n",
      " 12  Crossing                           353510 non-null  bool   \n",
      " 13  Give_Way                           353510 non-null  bool   \n",
      " 14  Junction                           353510 non-null  bool   \n",
      " 15  No_Exit                            353510 non-null  bool   \n",
      " 16  Railway                            353510 non-null  bool   \n",
      " 17  Roundabout                         353510 non-null  bool   \n",
      " 18  Station                            353510 non-null  bool   \n",
      " 19  Stop                               353510 non-null  bool   \n",
      " 20  Traffic_Calming                    353510 non-null  bool   \n",
      " 21  Traffic_Signal                     353510 non-null  bool   \n",
      " 22  Turning_Loop                       353510 non-null  bool   \n",
      " 23  Year                               353510 non-null  int64  \n",
      " 24  Month                              353510 non-null  int64  \n",
      " 25  DayOfWeek                          353510 non-null  int64  \n",
      " 26  State_AL                           353510 non-null  uint8  \n",
      " 27  State_AR                           353510 non-null  uint8  \n",
      " 28  State_AZ                           353510 non-null  uint8  \n",
      " 29  State_CA                           353510 non-null  uint8  \n",
      " 30  State_CO                           353510 non-null  uint8  \n",
      " 31  State_CT                           353510 non-null  uint8  \n",
      " 32  State_DC                           353510 non-null  uint8  \n",
      " 33  State_DE                           353510 non-null  uint8  \n",
      " 34  State_FL                           353510 non-null  uint8  \n",
      " 35  State_GA                           353510 non-null  uint8  \n",
      " 36  State_IA                           353510 non-null  uint8  \n",
      " 37  State_ID                           353510 non-null  uint8  \n",
      " 38  State_IL                           353510 non-null  uint8  \n",
      " 39  State_IN                           353510 non-null  uint8  \n",
      " 40  State_KS                           353510 non-null  uint8  \n",
      " 41  State_KY                           353510 non-null  uint8  \n",
      " 42  State_LA                           353510 non-null  uint8  \n",
      " 43  State_MA                           353510 non-null  uint8  \n",
      " 44  State_MD                           353510 non-null  uint8  \n",
      " 45  State_ME                           353510 non-null  uint8  \n",
      " 46  State_MI                           353510 non-null  uint8  \n",
      " 47  State_MN                           353510 non-null  uint8  \n",
      " 48  State_MO                           353510 non-null  uint8  \n",
      " 49  State_MS                           353510 non-null  uint8  \n",
      " 50  State_MT                           353510 non-null  uint8  \n",
      " 51  State_NC                           353510 non-null  uint8  \n",
      " 52  State_ND                           353510 non-null  uint8  \n",
      " 53  State_NE                           353510 non-null  uint8  \n",
      " 54  State_NH                           353510 non-null  uint8  \n",
      " 55  State_NJ                           353510 non-null  uint8  \n",
      " 56  State_NM                           353510 non-null  uint8  \n",
      " 57  State_NV                           353510 non-null  uint8  \n",
      " 58  State_NY                           353510 non-null  uint8  \n",
      " 59  State_OH                           353510 non-null  uint8  \n",
      " 60  State_OK                           353510 non-null  uint8  \n",
      " 61  State_OR                           353510 non-null  uint8  \n",
      " 62  State_PA                           353510 non-null  uint8  \n",
      " 63  State_RI                           353510 non-null  uint8  \n",
      " 64  State_SC                           353510 non-null  uint8  \n",
      " 65  State_SD                           353510 non-null  uint8  \n",
      " 66  State_TN                           353510 non-null  uint8  \n",
      " 67  State_TX                           353510 non-null  uint8  \n",
      " 68  State_UT                           353510 non-null  uint8  \n",
      " 69  State_VA                           353510 non-null  uint8  \n",
      " 70  State_VT                           353510 non-null  uint8  \n",
      " 71  State_WA                           353510 non-null  uint8  \n",
      " 72  State_WI                           353510 non-null  uint8  \n",
      " 73  State_WV                           353510 non-null  uint8  \n",
      " 74  State_WY                           353510 non-null  uint8  \n",
      " 75  Weather_Condition_Clear            353510 non-null  uint8  \n",
      " 76  Weather_Condition_Cloudy           353510 non-null  uint8  \n",
      " 77  Weather_Condition_Drizzle          353510 non-null  uint8  \n",
      " 78  Weather_Condition_Fair             353510 non-null  uint8  \n",
      " 79  Weather_Condition_Fog              353510 non-null  uint8  \n",
      " 80  Weather_Condition_Haze/Smoke/Dust  353510 non-null  uint8  \n",
      " 81  Weather_Condition_Mist             353510 non-null  uint8  \n",
      " 82  Weather_Condition_Partly Cloudy    353510 non-null  uint8  \n",
      " 83  Weather_Condition_Rain             353510 non-null  uint8  \n",
      " 84  Weather_Condition_Snow             353510 non-null  uint8  \n",
      " 85  Weather_Condition_Thunderstorm     353510 non-null  uint8  \n",
      " 86  Weather_Condition_Tornado          353510 non-null  uint8  \n",
      " 87  Weather_Condition_Wintry Mix       353510 non-null  uint8  \n",
      " 88  Astronomical_Twilight_Night        353510 non-null  uint8  \n",
      " 89  Hour                               353510 non-null  int64  \n",
      "dtypes: bool(13), float64(9), int64(5), uint8(63)\n",
      "memory usage: 66.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Drop the 'Time' and 'Start_Time' columns as their components have been extracted\n",
    "model_df = model_df.drop(['Time', 'Start_Time'], axis=1)\n",
    "model_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7de1397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "X = model_df.drop('Severity', axis=1)  # Features\n",
    "y = model_df['Severity']  # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d04f8935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=78)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afe299e",
   "metadata": {},
   "source": [
    "### I tried appliying the  SMOTE method to deal with the imbalance of the sample  only on training data, but it didn't work.  Leavign code for reference.\n",
    "#### smote = SMOTE()\n",
    "#### X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "666dca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance and scale the resampled training data and the testing data.  Leaving code for reference.\n",
    "#scaler = StandardScaler()\n",
    "#X_train_resampled_scaled = scaler.fit_transform(X_train_resampled)\n",
    "#X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create a StandardScaler instance and scale the training data and the testing data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8278da",
   "metadata": {},
   "source": [
    "## Try 3 different models from simpler to more complex.  Will start with a Logistic Regression, then a Decision Tree, and finally a Deep Neural Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a3bfb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.20      0.00      0.00       856\n",
      "           2       0.82      0.97      0.89     70658\n",
      "           3       0.48      0.17      0.25     14592\n",
      "           4       0.23      0.00      0.01      2272\n",
      "\n",
      "    accuracy                           0.80     88378\n",
      "   macro avg       0.43      0.28      0.29     88378\n",
      "weighted avg       0.74      0.80      0.75     88378\n",
      "\n",
      "[[    1   854     1     0]\n",
      " [    4 68308  2332    14]\n",
      " [    0 12128  2447    17]\n",
      " [    0  1983   280     9]]\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression analysis\n",
    "\n",
    "#lr_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "#lr_model.fit(X_train_resampled_scaled, y_train_resampled)\n",
    "\n",
    "lr_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "lr_model.fit(X_train_scaled, y_train)  # Use the original training data\n",
    "y_pred = lr_model.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf61b87",
   "metadata": {},
   "source": [
    "## add analysis for the Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5daae8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.41      0.42       856\n",
      "           2       0.89      0.88      0.89     70658\n",
      "           3       0.54      0.56      0.55     14592\n",
      "           4       0.19      0.21      0.20      2272\n",
      "\n",
      "    accuracy                           0.81     88378\n",
      "   macro avg       0.51      0.52      0.52     88378\n",
      "weighted avg       0.81      0.81      0.81     88378\n",
      "\n",
      "[[  355   378   120     3]\n",
      " [  365 62386  6333  1574]\n",
      " [   98  5881  8207   406]\n",
      " [   10  1361   426   475]]\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree model\n",
    "\n",
    "#leaving code for the resampled set for reference.\n",
    "#dt_model = DecisionTreeClassifier(random_state=78)\n",
    "#dt_model.fit(X_train_resampled_scaled, y_train_resampled)\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=78)\n",
    "dt_model.fit(X_train_scaled, y_train)  # Use the original training data\n",
    "y_pred = dt_model.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6925cb",
   "metadata": {},
   "source": [
    "## add analysis for the decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1050558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               9000      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 404       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19504 (76.19 KB)\n",
      "Trainable params: 19504 (76.19 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Deep Neural Model\n",
    "# Convert labels to categorical one-hot encoding\n",
    "y_one_hot = tf.keras.utils.to_categorical(y - 1)  # -1 because to_categorical assumes classes start at 0\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.25, random_state=78)\n",
    "\n",
    "# Define the model\n",
    "number_input_features = len(X_train_scaled[0])\n",
    "hidden_nodes_layer1 = 100\n",
    "hidden_nodes_layer2 = 100\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "model.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, activation='relu', input_dim=number_input_features))\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "model.add(tf.keras.layers.Dense(units=y_train.shape[1], activation='softmax'))\n",
    "\n",
    "# Check the structure of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03e0ab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f257569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5303/5303 [==============================] - 20s 4ms/step - loss: 0.3791 - accuracy: 0.8380\n",
      "Epoch 2/20\n",
      "5303/5303 [==============================] - 21s 4ms/step - loss: 0.3784 - accuracy: 0.8383\n",
      "Epoch 3/20\n",
      "5303/5303 [==============================] - 22s 4ms/step - loss: 0.3768 - accuracy: 0.8386\n",
      "Epoch 4/20\n",
      "5303/5303 [==============================] - 21s 4ms/step - loss: 0.3761 - accuracy: 0.8387\n",
      "Epoch 5/20\n",
      "5303/5303 [==============================] - 21s 4ms/step - loss: 0.3753 - accuracy: 0.8393\n",
      "Epoch 6/20\n",
      "5303/5303 [==============================] - 21s 4ms/step - loss: 0.3739 - accuracy: 0.8401\n",
      "Epoch 7/20\n",
      "5303/5303 [==============================] - 20s 4ms/step - loss: 0.3734 - accuracy: 0.8405\n",
      "Epoch 8/20\n",
      "5303/5303 [==============================] - 21s 4ms/step - loss: 0.3724 - accuracy: 0.8409\n",
      "Epoch 9/20\n",
      "5303/5303 [==============================] - 21s 4ms/step - loss: 0.3710 - accuracy: 0.8410\n",
      "Epoch 10/20\n",
      "5303/5303 [==============================] - 20s 4ms/step - loss: 0.3706 - accuracy: 0.8413\n",
      "Epoch 11/20\n",
      "5303/5303 [==============================] - 20s 4ms/step - loss: 0.3698 - accuracy: 0.8414\n",
      "Epoch 12/20\n",
      "5303/5303 [==============================] - 20s 4ms/step - loss: 0.3694 - accuracy: 0.8417\n",
      "Epoch 13/20\n",
      "5303/5303 [==============================] - 19s 4ms/step - loss: 0.3687 - accuracy: 0.8415\n",
      "Epoch 14/20\n",
      "5303/5303 [==============================] - 21s 4ms/step - loss: 0.3676 - accuracy: 0.8422\n",
      "Epoch 15/20\n",
      "5303/5303 [==============================] - 21s 4ms/step - loss: 0.3673 - accuracy: 0.8421\n",
      "Epoch 16/20\n",
      "5303/5303 [==============================] - 21s 4ms/step - loss: 0.3665 - accuracy: 0.8424\n",
      "Epoch 17/20\n",
      "5303/5303 [==============================] - 21s 4ms/step - loss: 0.3657 - accuracy: 0.8433\n",
      "Epoch 18/20\n",
      "5303/5303 [==============================] - 21s 4ms/step - loss: 0.3650 - accuracy: 0.8433\n",
      "Epoch 19/20\n",
      "5303/5303 [==============================] - 20s 4ms/step - loss: 0.3649 - accuracy: 0.8431\n",
      "Epoch 20/20\n",
      "5303/5303 [==============================] - 23s 4ms/step - loss: 0.3641 - accuracy: 0.8435\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "fit_model = model.fit(X_train_scaled, y_train, epochs=20, batch_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4515a132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2762/2762 - 7s - loss: 0.4906 - accuracy: 0.8279 - 7s/epoch - 3ms/step\n",
      "Test accuracy: 0.8279436230659485\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11ad45f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2762/2762 [==============================] - 8s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.35      0.42       856\n",
      "           1       0.87      0.94      0.90     70658\n",
      "           2       0.59      0.44      0.50     14592\n",
      "           3       0.37      0.09      0.15      2272\n",
      "\n",
      "    accuracy                           0.83     88378\n",
      "   macro avg       0.58      0.46      0.49     88378\n",
      "weighted avg       0.81      0.83      0.81     88378\n",
      "\n",
      "[[  303   493    57     3]\n",
      " [  224 66275  3963   196]\n",
      " [   64  7987  6388   153]\n",
      " [    8  1715   343   206]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predict the classes for the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Convert the predictions from one-hot encoded vectors to the class labels\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Convert the true test set labels from one-hot encoded vectors to the class labels\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Generate the classification report\n",
    "clf_report = classification_report(y_true_classes, y_pred_classes)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "print(clf_report)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb9e792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
